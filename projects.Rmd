---
title: "ML Projects and Thermodynamics Research"
output:
   html_document:
    toc: true
    toc_float: true
      
    
    
---

***
## **Machine Learning**

<hr style="border:2px solid gray"> </hr>
### MRI Image Segmentation[<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/Brain-MRI-Segmentation-U-nets)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="65%",out.height="65%",fig.cap="Fig 1 Example 2D slice(s) of the 3D brain image showing both a normal subject (top) and a subject with glioma (bottom). Ref: CS446 UIUC",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("image1.png"))
```
  - Gliomas are tumors of the brain that involve glial cells in the brain. Gliomas are classified as grades I to IV, where the grades indicate severity of the diseases. 
  - The categories are:
    - Grade I: benign, curable with complete surgical resection, 
    - Grade II: low grade, undergo surgical resection, radiotherapy/chemotherapy,
    - Grade III/IV: high-grade glioma’s, and
    - Grade IV: glioblastoma. 
  - The task is to identify the location of the tumor, and its classification into three groups, namely:
    - edema which indicates inflammation, 
    - enhancing which indicates part of the tumor with active growth , and
    - the necrotic core which are dead tissue, generally in the center).
  - Four different magnetic resonance images (MRI), also known as contrasts are provided for each subject, namely,
    - T1-weighted which this tends to be higher intensity with tissues with more lipids,
    - T2-weighted which is usually higher intensity for tissues with more water,
    - FLAIR this is similar to T2 but free water is suppressed, and
    - T1CE this is same as T1, but with a contrast agent injected to brighten certain patterns).
  - After training on labelled images the task was to to predict the segmentation of unlabelled images in the test set. This is also known as semantic segmentation in computer vision and this project was carried out as a part of CS 446: Machine Learning at UIUC.
  - For more details and codes please refer to [<span style="color:red">here.</span>](https://github.com/punitjha/Brain-MRI-Segmentation-U-nets) 
  - All calculations were run on Google Cloud. 
  - A few slices from results are shown below:
```{r, echo=FALSE, out.width="75%",out.height="75%",fig.cap="Results obtained from the U-net model",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("mri_016.png"))
```  

<!-- Each sample is a tensor of size = 4 x height (H) x width (W) x depth (D), where the four 3D tensors represent the a) native (T1) and b) post-contrast T1-weighted (T1Gd), c) T2-weighted (T2), and d) T2-FLAIR contrasts (images) of the brain MRI. Note that different samples may have slightly different dimensions H, W, and D -- your are models should be able to handle this. For training samples, you are also provided with a HxWxD label “image”, with a label for each voxel (3D pixel). Your task is to predict the segmentation of unlabelled images. This is also known as semantic segmentation in computer vision. Beyond the course notes, you can find some straightforward descriptions here, and here Note that the output dimension for each sample is of size H x W x D. -->
<!-- Learning Goals -->

<!--     Gain real-world experience with ML, subject to blind test evaluation -->
<!--     Investigate the effect of spatially correlated features. The use of convolution and other image processing techniques may be helpful. -->
<!--     Apply ML with high dimensional inputs and outputs -->
<!--     Grapple with data preprocessing/cleaning, if necessary -->

<!-- Data -->

<!-- The train and validation split of the dataset are provided on box. You are provided with 204 labeled (training) samples and 68 unlabelled (validation) images, all saved as NumPy tensors (.npy). Download data here (~3GB): -->
<hr style="border:2px solid gray"> </hr>
### Original Research Proposal[<span style="color:blue">[Report]</span>](https://drive.google.com/file/d/1Q4W8AkKSU9xhT38bK1Kor9uf_cKh4fRC/view?usp=sharing)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap="Results obtained from the U-net model. Ref.: N. Artrith, T. Morawietz, and J. Behler, Phys. Rev. B 83, 153101 (2011)",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("medium.png"))
```  

  - ORP titled, **On the inclusion of long-range interactions among molecules in machine learning models**.
  - Developed an ORP on the problem of inclusion of long-range (electrostatic) interactions in machine learning algorithms. Analyzed different neural network schemes, especially deep high-dimensional neural networks (HDNNs), and whether they can efficiently learn and predict molecular charges learned from various charge partitioning schemes.
  - I studied different charge partitioning schemes like Hirshfeld, Charge Model 5, Merz-Singh-Kollman, and Natural Bonding Orbital methods used in molecular dynamics simulations.
  - Proposed modifications to the existing mathematical formulation and structure of HDNNs to be able to better predict  molecular charges, the computational cost for implementing the project, potential  setbacks, and alternate plans for the project.
  - Please find the details of my ORP report  [here](https://drive.google.com/file/d/1Q4W8AkKSU9xhT38bK1Kor9uf_cKh4fRC/view?usp=sharing). 

    <!-- - More details [here](https://punitjha.github.io/orp_project.html). -->

<hr style="border:2px solid gray"> </hr>
### YOLO Object Detection on PASCAL VOC[<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/YOLO-Object-Detection)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap="Results obtained from a trained YOLO model",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("download_lol.png"))
```  
  -  Implemented a YOLO-like object detector on the PASCAL VOC 2007 dataset to produce results like in the above image.
  -  Used a pre-trained network structure for the model.  In particular, we used the ResNet50 architecture as a base for our detector. This is different from the base architecture in the YOLO paper and also results in a different output grid size (14x14 instead of 7x7).
  -  Please find the details of the Python code used  [here](https://github.com/punitjha/YOLO-Object-Detection). 
  -  All calculations were run on Google Colab.



<!-- ### **Face Generation with GANs** -->
<hr style="border:2px solid gray"> </hr>
### Face Generation with GANs[<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/Generative-Adversarial-Network)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 95%",out.height="95%",fig.cap=" Deep Convolutional Generative Adversarial Network Architecture. Ref: https://gluon.mxnet.io/chapter14_generative-adversarial-networks/dcgan.html",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("dcgan.png"))
``` 

  -  Trained a generative adversarial network (GAN) with spectral normalization of discriminator weights on the CelebA Dataset and generate facial images.
  -  The generator and discriminator network architectures I implement are roughly based on DCGAN.
  -  Spectral normalization of discriminator weights is used to improve the quality of generator images.
  -  Both DCGAN and LSGAN loss functions were used keeping the network architecture constant (with and without spectral normalization).
  -  Please find the details of the Python code used  [here](https://github.com/punitjha/YOLO-Object-Detection). 
  -  All calculations were run on Google Colab. Due to limitaions in availability of computational resources, I could train the model only for 22 epochs. Some results are shown below.



```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap=" Results obtained from LSGAN and DSGAN",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("gan_results_1.png"))
```
```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap="Effect of spectral normalization on the quality of predicted images ",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("spec_norm.png"))
```




<hr style="border:2px solid gray"> </hr>
<!-- ### **Text Generation & Language classification (RNNs)** -->
### Text Generation & Language classification (RNNs)[<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/Recurrent-Neural-Networks)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 45%",out.height="45%",fig.cap="Vizualizing the performance of ourRNN  model by creating a confusion matrix. The ground truth languages of samples are represented by rows in the matrix while the predicted languages are represented by columns.",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("confusion.png"))
```

  - I train an RNN for two tasks on text data for 2 puposes:
    - Language classification - RNN will learn to detect which language a chunk of text is written in (similar to a feature you might find in an online translator). While this might be relatively easy to do if the input text is unicode and unique characters indicate a particular language, I address the case where all input text to converted to ASCII characters so our network must learn instead to detect letter patterns. The model was trained on the Bible which is a large text translated to different languages but in the same easily parsable format, so I used 20 different copies of the Bible in different languages obtined from Project Gutenberg.
    - Text generation - RNN learns to generate text by predicting the most likely next character based on previous characters. The model consists of three layers 
      - a linear layer that encodes the input character into an embedded state, 
      - an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and
      - a decoder layer that outputs the predicted character scores distribution.
    - The model was trained on the completer works of Shakespheare and 24 different novels of Charles Dickens. 
    
**Sample output generated by the RNN after training on Shakespheare is shown below.**

```
TRIA:
He dew that with merry a man for the strange.
I then to the rash, so must came of the chamuness, and that I'll treason dost
the heaven! how there. The run of these thou instress
Which wast true come come on my tongue.

KATHARINE:
My lord, the crown English am a thanks, and I
have you weep you galls. O, I wast thy change;
And go turn of my love to the master.'

ARCHBISHOP OF YORK:
I'll find by dogs, noble.

SAMLET:
The matter were be true and treason
Free supples'd best the soldiered.

TITUS ANDRONICUS:
I ever a bood;
But one a stand have a court in thee: which man as thy break on my bed
'As oath a women; there and shake me; and whencul, comes the house
For them he wall; and no live away. Fies, sir.
```
<hr style="border:2px solid gray"> </hr>
### Multi Label Image Classification[<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/Multi-Label-Image-Classification)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 45%",out.height="45%",fig.cap="A sample image from the PASCAL VOC 2007  dataset",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("pascal_voc.png"))
```


  - I implement a multi-label image classifier on the [<span style="color:blue">PASCAL VOC 2007 </span>](http://host.robots.ox.ac.uk/pascal/VOC/voc2007) dataset. I basically run three experiments
    - Train AlexNet (PyTorch built-in) from scratch.
    - Fine-tune AlexNet (PyTorch built-in), which is pretrained on [<span style="color:blue">ImageNet.</span>](http://www.image-net.org/)
    - Train a simple network (defined in the file classifier.py) from scratch.
  - In the second part, I design and train my own deep simple convolutional network to predict a binary present/absent image-level label for each of the 20 PASCAL classes. I used AlexNet as the starting point. The goal was to get an average mAP socre of atleast 40 percent. 
  
**The average precision on different classes are shown below after 55 epochs on Google Cloud (16GB RAM and 1 NVIDIA K80 GPU).**

```
-------  Class: aeroplane        AP:   0.6468  -------
-------  Class: bicycle          AP:   0.4247  -------
-------  Class: bird             AP:   0.3491  -------
-------  Class: boat             AP:   0.3989  -------
-------  Class: bottle           AP:   0.1596  -------
-------  Class: bus              AP:   0.2318  -------
-------  Class: car              AP:   0.6456  -------
-------  Class: cat              AP:   0.3552  -------
-------  Class: chair            AP:   0.4179  -------
-------  Class: cow              AP:   0.2235  -------
-------  Class: diningtable      AP:   0.3586  -------
-------  Class: dog              AP:   0.3028  -------
-------  Class: horse            AP:   0.6846  -------
-------  Class: motorbike        AP:   0.5332  -------
-------  Class: person           AP:   0.7901  -------
-------  Class: pottedplant      AP:   0.2159  -------
-------  Class: sheep            AP:   0.2858  -------
-------  Class: sofa             AP:   0.2924  -------
-------  Class: train            AP:   0.5996  -------
-------  Class: tvmonitor        AP:   0.2998  -------
mAP: 0.4108
Avg loss: 0.1801034240768506
```
<hr style="border:2px solid gray"> </hr>
### Multi Layer Neural Networks[<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/Multi-Layer-Neural-Networks)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 45%",out.height="45%",fig.cap="A sample image from the CIFAR-10 image classification dataset. Ref: https://www.cs.toronto.edu/~kriz/cifar.html",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("cifar_10.png"))
```
  - I implemented multi-layer neural networks from scratch on the [<span style="color:blue"> CIFAR-10 </span>](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, to understand the fundamentals of neural networks and backpropagation.

  - I wrote from scratch forward and backward pass and trained two- and three-layer networks with SGD and Adam optimizers.
  
  - Had experience with hyperparameter tuning and using proper train/test/validation data splits.
  

```{r, echo=FALSE, out.width=" 75%",out.height="75%",fig.cap="Results of using a the SGD and Adam optimzers on the CIFAR-10 dataset. Note that Adam converges faster than SGD in the loss history plot which is expected. We also see overfitting in the classification plot given that our dataset was realtively small compared to the number of NN parameters.",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("adam.png"))
```



<hr style="border:2px solid gray"> </hr>
### Ad Hoc Information Retrieval System[<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/tree/master/NLP_Lab5)
<hr style="border:2px solid gray"> </hr>
<!-- ### **Ad Hoc Information Retrieval System**   -->
```{r, echo=FALSE, out.width=" 65%",out.height="65%",fig.cap=" Architecture of an ad hoc IR system. Source: Jurafsky and Martin 2009, sec. 23.2",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("info.png"))
```
  - Using 1400 abstracts of journal articles created an information retrieval system that takes in a query and outputs the top matched abstracts using TF-IDF weights and cosine similarity scores.
  - The model was trained on the  [<span style="color:blue"> Cranfield collection </span>](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  This was the pioneering test collection in allowing precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments. Collected in the United Kingdom starting in the late 1950s, it contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs.
  - This project was a part of the ECE 365: Data Science and Engineer course.
  - More details of the project can be found in this repository [here](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/tree/master/NLP_Lab5)

<hr style="border:2px solid gray"> </hr>
### Machine Learning Algorithms Using only Numpy
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="500%",out.height="500%",fig.cap=" (a) Comparison of SGD with Momentum or Nesterov Accelerated Gradient, and Adam methods and their rate of convergence. (b) Image compression using PCA",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("index_both.png"))
```

- I implemented a few ML algorithms from scratch using only Python Numpy. All of these algorithms are run on tiny datasets on a standard CPU laptop. These were part of assigments from CS-446 and CS-498.
  - Stochastic Gradient Descent Variants - SGD with Momentum or Nesterov Accelerated Gradient, and Adam. [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - GMM [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - HMM [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - PCA [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - RNN [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - Adaboost On Stumps [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - Feed Forward Neural Network [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - kNN, LDA and Bayse Classifiers and comparision with corresponding scikit-learn modules. [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab2_vvv/punit2_lab2.ipynb)
  - Mulit-layer Neural Networks (2 and 3 layered NN with SGD and Adam) [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/Multi-Layer-Neural-Networks)
  - Logistic regression   [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/Linear-Classifiers)
  - Perceptron   [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/Linear-Classifiers)
  - SVM   [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/Linear-Classifiers)
  - Softmax  [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/Linear-Classifiers)
 

<hr style="border:2px solid gray"> </hr>
###  Machine Learning, NLP, and Text Analysis using Scikit-learn
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="200%",out.height="200%",fig.cap="Use of [spectral clustering](https://arxiv.org/pdf/0711.0189.pdf) to separate data that K-means cannot. (a) Original data (b) Clustering using K-means (c) Spectral Clustering",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("download.png"))
```

  - **Machine Learning**
      <!-- - k-Nearest Neighbors, Bayes Classifiers, and Linear Discriminant Analysis. [[Jupyter Notebook Link]]() -->
      - Naive Bayes, Logistic Regression, and Support Vector Machines. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab3_vvv/punit2_lab3.ipynb)
      - K-means clustering, Vector Quantization, Nearest Neighbors Classification, and Linear Regression. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab4_vvv/punit2_lab4.ipynb)
      - Eigen decomposition, Singular Value Decomposition, and Principal Component Analysis.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab5_vvv/punit2_lab5.ipynb)
     - Intro to Octave, Linear regression with one variable, Gradient Descent, and Linear regression with multiple variables.[[Octave ]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex1/ex1)
    - Logistic Regression, Regularized logistic regression, and Plotting the decision boundary. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex2/ex2)
    - Multi-class Classification/ One-vs-all Classification and Neural Networks. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex3/ex3)
    - Neural Networks, Backpropagation, and Visualizing the hidden layer. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex4/ex4)
    - Regularized Linear Regression, Bias-variance and learning curves, Polynomial regression, Adjusting the regularization parameter, Computing test set error, and visualization.
[[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex5/ex5)
    - Support Vector Machines and Spam Classification. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex6/ex6)
    - K-means Clustering and Image compression with K-means.
[[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex7/ex7)
  - **Natural Language Processing**
      - Text processing and regular expressions, word frequencies and language, manipulate corpora and plot insightful graphs. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab1/punit2/Lab1/punit2.ipynb)
      - Tokenization, bag-of-words represenation, ogistic regression classifier (discriminative model) using PyTorch.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab2/punit2/NLPLab2/Lab2_NLP.ipynb)
      - N-gram model, generate representative sentences, study of the effect of training data size, and language model complexity (n-gram size), on the modeling capacity of a language model.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab3/punit2/NLPLab3/NLPLab3.ipynb)
      - Count-Based Word Vectors, Co-occurrence_matrix, PCA, plot-embeddings,  Word2Vec Word Embeddings, Cosine Similarity, Polysemous Words, Synonyms & Antonyms, Solving Analogies with Word Vectors, Guided Analysis of Bias in Word Vectors, and Independent Analysis of Bias in Word Vectors. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab4/punit2/Lab4/lab4.ipynb)
  - **Text Analysis**
      - Syntagmatic word association mining.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-1/punit2_lab1.ipynb)
      - Sentiment analysis.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-2/punit2_lab2.ipynb)
      - Document Clustering using TF-IDF vectorizer.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-3/punit2_lab3.ipynb)
      - Topic Modelling: Use of gensim library to create a LDA topic model.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-4/punit2_lab4.ipynb)
    

 

***
## **Programming Projects**
<hr style="border:2px solid gray"> </hr>

### Parallel Programming and C++ Projects
<hr style="border:2px solid gray"> </hr>


```{r, echo=FALSE, out.width="100%",out.height="100%",fig.cap="",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("parallel_1.png"))
```

- **Design and Implementations of Parallel OpenMP and MPI LU factorization: Application to Finite Difference Method.**
  - In this project, I with my group presented  six different OpenMP and
MPI based parallel algorithms for the LU factorization of dense matrices.
  - Please find the details [here](https://drive.google.com/file/d/1I17RTZGtopQQOAkk9z2MUyQEGYWmYsRN/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_Project)
     
- **Optimization of a dense matrix-matrix multiplication routine and understanding its performance behavior.** 
  - Parallel programming machine project on dense matrix-matrix multiplication and cache performance.
  - Please find the details [here](https://drive.google.com/file/d/1IvqxqR6E6_CINKB8OunAmBpQvYcb_nPo/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_MPs)
   
- **Utilization of  multi-core CPU using OpenMP and MPI to implement a dense matrix-matrix multiply (MM) routine and understanding the performance behavior.**
  - Usage of PAPI to measure cache misses and to analyze the performance.
  - Please find the details [here](https://drive.google.com/file/d/1yYzdTwtnV8-4xoFCRmFnWe275Ny0dOx9/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_MPs)
    
- **Utilization of  multi-core CPU using OpenMP and a GPU using OpenACC to implement a matrix-multiply routine for multiplying a band matrix by a dense matrix.**
  - Investigation of the performance impact of varying the band and matrix size.
  - Please find the details [here](https://drive.google.com/file/d/1Pb781ThU_9EG2uDTm1NB4uaETlWU7GH4/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_MPs)
   
<hr style="border:2px solid gray"> </hr>
### Chemistry Programming Projects
<hr style="border:2px solid gray"> </hr>

<!-- 1. **Programming projects for [CHEM 548: Molecular Electronic Structure Advanced Quantum Chemistry & Numerical Methods.](http://faculty.scs.illinois.edu/hirata/education.html)** -->
<!--    - Third Project -->
<!--      - Hückel and Su-Schrieffer-Heeger band structures for 1D and 2D solids. -->
<!--      - The report can be found [here](https://drive.google.com/file/d/13CjQVDkpwSTJEk4sAuKDva8kw8z89CHt/view?usp=sharing) -->
<!--      - Codes can be found [here](https://github.com/punitjha/CHEM_548_Project_3) -->
<!--    - Second Project -->
<!--      - Wave packet propagation, phase and group velocities, Ehrenfest theorem, convolution theorem, path integrals. -->
<!--      - The report can be found [here](https://drive.google.com/file/d/1_XcSwoqlycy-i354jd4TW_kfLtEWTs8P/view?usp=sharing) -->
<!--      - Codes can be found [here](https://github.com/punitjha/CHEM_548_Project_2) -->
<!--    - First Project -->
<!--      - Time-dependent perturbation theory and spectroscopy. -->
<!--      - The report can be found [here](https://drive.google.com/file/d/1zz3pD4KzFYVm5nen6yhf8u-hqnGXjfF5/view?usp=sharing) -->
<!--      - Codes can be found [here](https://github.com/punitjha/CHEM_548_Project_1) -->

```{r, echo=FALSE, out.width="80%",out.height="80%",fig.cap="",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("chem.png"))
```

- **Electronic structure theory programming projects. ** 
   - Molecular Geometry Analysis.
     - [Instructions](https://drive.google.com/file/d/1ucsnJL2St-DKmW5cs6o_3M0TKxbly09p/view?usp=sharing) and  [Codes](https://github.com/punitjha/Crawdad_Project_4)
   - Harmonic Vibrational Analysis.
     - [Instructions](https://drive.google.com/file/d/1rTu-TCD8vZMX0CsE9NFp1brANDxAqt5x/view?usp=sharing) and  [Codes](https://github.com/punitjha/Crawdad_Project_3)
   - The Hartree-Fock Self-Consistent Field Method.
     -  [Instructions](https://drive.google.com/file/d/1sIRmEjvBEmPc7N1bsYVXyuu1kVwnSsYz/view?usp=sharing) and [Codes](https://github.com/punitjha/Crawdad_Project_2)
   - The Second-Order Möller-Plesset Perturbation Theory (MP2) Energy.
     -  [Instructions](https://drive.google.com/file/d/1zDiOA2GHTFlNxmCZSjk4dJDEQ-xe4-4x/view?usp=sharing) and [Codes](https://github.com/punitjha/Crawdad_Project_1)
   - Hückel and Su-Schrieffer-Heeger band structures for 1D and 2D solids.
     - [Report](https://drive.google.com/file/d/13CjQVDkpwSTJEk4sAuKDva8kw8z89CHt/view?usp=sharing) and  [Codes](https://github.com/punitjha/CHEM_548_Project_3)
   - Wave packet propagation, phase and group velocities, Ehrenfest theorem, convolution theorem, path integrals.
     - [Report](https://drive.google.com/file/d/1_XcSwoqlycy-i354jd4TW_kfLtEWTs8P/view?usp=sharing) and  [Codes](https://github.com/punitjha/CHEM_548_Project_2)
   - Time-dependent perturbation theory and spectroscopy.
     - [Report](https://drive.google.com/file/d/1zz3pD4KzFYVm5nen6yhf8u-hqnGXjfF5/view?usp=sharing) and [Codes](https://github.com/punitjha/CHEM_548_Project_1)




***
## **Chemistry Research Projects and Summer Internships**
<hr style="border:2px solid gray"> </hr>
### Ph.D. Research Project 
<hr style="border:2px solid gray"> </hr>

  - Research conducted at the University of Illinois at Urbana-Champaign, Urbana-Champaign, IL, Aug 2016 - Present. 
     - **Topic**: Finite-temperature many-body perturbation theory.
     - More details [here](https://punitjha.github.io/phd_project.html).
    
<hr style="border:2px solid gray"> </hr>
### M. Sc. Research Project
<hr style="border:2px solid gray"> </hr>

- Research conducted at the National Institute of Science Education and Research, Bhubaneswar, India, Aug 2014 - June 2016.
    - **Topic**: Diagonal suppressed constant time Correlation Spectroscopy (COSY).
    - More details [here](https://punitjha.github.io/msc_project.html). 
    
<hr style="border:2px solid gray"> </hr>
### Research Summer Internships
<hr style="border:2px solid gray"> </hr>
1. **Undergraduate Summer Intern- 2015**
    - Research conducted at the University of Calgary, Calgary, Canada, May 2015 - Aug 2015.
       - **Topic**: Dynamics of gas-hydrate nucleation.
       -  More details [here](https://punitjha.github.io/calgary.html).
  
2. **Undergraduate Summer Intern- 2014**
     - Research conducted at the Indian Institute of Technology- Bombay, Mumbai, India, May 2014 - Aug 2014.
        - **Topic**: Quantum reactive flux theory.
        - More details [here](https://punitjha.github.io/iit_b.html).
         
      
3.  **Undergraduate Summer Intern- 2013** 
     - Research conducted at the National Institute of Science Education and Research, Bhubaneswar, India, May 2013 - Aug 2013.
         - **Topic**: Reactive molecular dynamics of hydrocarbon dissociation on Ni surface.
         - More details [here](https://punitjha.github.io/niser_summer.html).
          
*** 

### References
  - ECE 365: Data Science & Eng.
  - CS 420: Parallel Programming
  - CS 446: Machine Learning
  - CS 498: Intro. to Deep Learning 
  - Data Mining Specialization 
  - Cloud Computing Specialization  
  - Machine Learning by Andrew Ng (Coursera)
  - CS 400: Data Structures in C++
  - Quantum Mech. & Spectrosc.
  - Molecular Electronic Structure
  - Quantum Mechanics
  - Statistical Thermodynamics
  
  
  
  <!--   - Reactive molecular dynamics of hydrocarbon dissociation on Ni surface: Carried out reactive dynamics simulations of the adsorption and decomposition of CH\textsubscript{4} molecules interacting with a Ni(100) surface. Studied the temperature dependence of chemisorption of CH\textsubscript{4} by breakage of C-H bonds that leads to the formation of CH\textsubscript{3} and H radical on a Ni(100) surface. -->
  <!-- - **Skills Acquired:** -->
  <!--         Learned rogramming in FORTRAN 90 and how to carry out quantum mechanical simulations using ReaxFF. -->
             
             
             
<!--     - Finite-temperature many-body perturbation theory (FT-MBPT): FT-MBPT is a theory to calculate the thermodynamic parameters of the grand canonical system at a given temperature.  -->
  <!--     - I implemented the $\lambda$-variation method to  calculate the $n$th-order corrections to grand potential and internal energy using the FT-MBPT formalism which showed it is mathematically inconsistent and does not conserve charge.  -->
  <!--     - Derived the correct analytic formulas for FT-MBPT using another ansatz wherein charge was conserved. -->
  <!--     Implemented the FT-MBPT in a canonical ensemble and derived semi-analytic formulas. -->
  <!-- - **Original Research Proposal (ORP)** -->
  <!--       - Developed an ORP on the problem of inclusion of long-range (electrostatic) interactions in machine learning algorithms. Analyzed different neural network schemes,            especially deep high-dimensional neural networks (HDNNs), and whether they can efficiently learn and predict molecular charges learned from various charge partitioning schemes. -->
  <!--     - Studied different charge partitioning schemes like Hirshfeld, Charge Model 5, Merz-Singh-Kollman, and Natural Bonding Orbital methods used in molecular dynamics              simulations. -->
  <!--     - Proposed modifications to mathematical formulation and structure of HDNNs to be able to better predict  molecular charges, computational cost for implementing the            project, potential  setbacks, and alternate plans for the project. -->
  <!-- - **Skills Acquired:** -->
  <!--         Acquired knowledge of \textit{ab initio} and DFT electronic structure methods. Learned about machine learning algorithms. Learned to program in Python and enhanced           my programming skills in C++ and FORTRAN. -->

  <!-- ---     -->
  
  
  
  
  
  <!--   - Diagonal suppressed constant time Correlation Spectroscopy (COSY): Worked on a novel NMR pulse sequence to achieve diagonal suppressed COSY (correlation spectroscopy) spectra with high sensitivity and absorptive cross peak line-shapes.              -->
  <!--   - Carried out simulations and  product operator  calculations for 2, 3, and 4 spin systems using the novel NMR pulse sequence. -->
  <!--   - Implemented various optimization strategies for the pulse sequence that included the addition of water suppression module, removing axial and multiple quantum peaks by using gradient fields, etc.  -->
  <!-- - **Skills Acquired:** -->
  <!--         Acquired programming skills to program in the TopSpin 2.1 programming language and  learned to how to perform 1D, 2D NMR experiments on a 400 MHz spectrometer. -->

  <!-- ---     -->
  
  
   <!-- - Quantum reactive flux theory: Studied the quantum analog of classical non-Markovian Kramers' equation and the phase space distribution function formulation of the method of reactive flux in the memory friction regime. Analyzed quantum Kramers' equation for energy diffusion and barrier crossing dynamics in the high-friction regime. -->
   <!--  - **Skills Acquired:** -->
   <!--        Acquired skills to use Mathematica for solving integrals and numerical analysis. Learned mathematical methods in physics. -->

   <!--   ---   -->
   
<!--      -->
<!--       - Dynamics of gas-hydrate nucleation: Wrote C++  codes to study the probability distribution of lifetime of gas hydrate cages in the molecular dynamics simulation of  incipient hydrates starting from a CH\textsubscript{4}/H\textsubscript{2}S bubble in liquid H\textsubscript{2}O in a 3D box. -->
<!--       - Studied the effect of the different guest gas molecules on the lifetime of these cages. } -->
<!--   - **Skills Acquired:** -->
<!--           Learned about the object-oriented programming features of C++ and its data structures. I also learned the skills -->
<!-- required to create memory-efficient C++ codes. -->

<!--   ---   -->
  
 
---
title: "ML Projects and Thermodynamics Research"
output:
   html_document
---

***
## **Machine Learning**

<hr style="border:2px solid gray"> </hr>
### MRI Image Segmentation[<span style="color:blue"> [GitHub]</span>](https://github.com/punitjha/Brain-MRI-Segmentation-U-nets)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="65%",out.height="65%",fig.cap="Fig 1 Example 2D slice(s) of the 3D brain image showing both a normal subject (top) and a subject with glioma (bottom). Ref: CS446 UIUC",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("image1.png"))
```
  - Gliomas are tumors of the brain that involve glial cells in the brain. Gliomas are classified as grades I to IV, where the grades indicate the severity of the diseases. The categories are:
    - Grade I: benign, curable with complete surgical resection, 
    - Grade II: low grade, undergo surgical resection, radiotherapy/chemotherapy,
    - Grade III/IV: high-grade gliomas, and
    - Grade IV: glioblastoma. 
  - The task is to identify the location of the tumor, and its classification into three groups, namely:
    - edema which indicates inflammation, 
    - enhancing which indicates part of the tumor with active growth, and
    - the necrotic core which are dead tissue, generally in the center.
  - Four different magnetic resonance images (MRI), also known as contrasts are provided for each subject,
    - T1-weighted which tends to be higher intensity with tissues with more lipids,
    - T2-weighted which is usually higher intensity for tissues with more water,
    - FLAIR this is similar to T2 but free water is suppressed, and
    - T1CE is the same as T1, but with a contrast agent injected to brighten certain patterns.
  - After training a U-net model of input dimension 192x163x4 on labeled images the task was to predict the segmentation of unlabelled images in the test set.
  - This is called semantic segmentation in computer vision and this project was carried out as a part of CS 446: Machine Learning at UIUC.
  - For more details and codes please refer to [<span style="color:blue">here.</span>](https://github.com/punitjha/Brain-MRI-Segmentation-U-nets) 
  - All calculations were run on Google Cloud. 
  - A few slices from the results are shown below:
```{r, echo=FALSE, out.width="75%",out.height="75%",fig.cap="Results obtained from the U-net model",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("mri_016.png"))
```  

<!-- Each sample is a tensor of size = 4 x height (H) x width (W) x depth (D), where the four 3D tensors represent the a) native (T1) and b) post-contrast T1-weighted (T1Gd), c) T2-weighted (T2), and d) T2-FLAIR contrasts (images) of the brain MRI. Note that different samples may have slightly different dimensions H, W, and D -- your are models should be able to handle this. For training samples, you are also provided with a HxWxD label “image”, with a label for each voxel (3D pixel). Your task is to predict the segmentation of unlabeled images. This is also known as semantic segmentation in computer vision. Beyond the course notes, you can find some straightforward descriptions here, and here Note that the output dimension for each sample is of size H x W x D. -->
<!-- Learning Goals -->

<!--     Gain real-world experience with ML, subject to blind test evaluation -->
<!--     Investigate the effect of spatially correlated features. The use of convolution and other image processing techniques may be helpful. -->
<!--     Apply ML with high dimensional inputs and outputs -->
<!--     Grapple with data preprocessing/cleaning, if necessary -->

<!-- Data -->

<!-- The train and validation split of the dataset are provided on box. You are provided with 204 labeled (training) samples and 68 unlabeled (validation) images, all saved as NumPy tensors (.npy). Download data here (~3GB): -->

<hr style="border:2px solid gray"> </hr>
### Original Research Proposal and Literature Review [<span style="color:blue">[Report]</span>](https://drive.google.com/file/d/1Q4W8AkKSU9xhT38bK1Kor9uf_cKh4fRC/view?usp=sharing)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap="Results obtained from the U-net model. Ref.: N. Artrith, T. Morawietz, and J. Behler, Phys. Rev. B 83, 153101 (2011)",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("medium.png"))
```  

  - ORP titled, **On the inclusion of long-range interactions among molecules in machine learning models**.
  - Developed an ORP on the problem of inclusion of long-range (electrostatic) interactions in ML and DL algorithms. Analyzed different neural network schemes, especially deep high-dimensional neural networks (HDNNs), and whether they can efficiently learn and predict molecular charges learned from various charge partitioning schemes.
  - Studied different charge partitioning schemes like Hirshfeld, Charge Model 5, Merz-Singh-Kollman, and Natural Bonding Orbital methods used in molecular dynamics simulations.
  - Proposed modifications to the existing mathematical formulation and structure of HDNNs to be able to better predict  molecular charges, the computational cost for implementing the project, potential  setbacks, and alternate plans for the project.
  - Please find the details of my ORP report  [here](https://drive.google.com/file/d/1Q4W8AkKSU9xhT38bK1Kor9uf_cKh4fRC/view?usp=sharing). 

    <!-- - More details [here](https://punitjha.github.io/orp_project.html). -->


<!-- ### **Deep Reinforcement Learning** -->
<hr style="border:2px solid gray"> </hr>
### Deep Reinforcement Learning[<span style="color:blue"> [GitFront]</span>](https://gitfront.io/r/user-2013142/0fbc0374f87206739e93444c1e6e3a8750c93eef/Deep-Reinforcement-Learning/)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 45%",out.height="45%",fig.cap="  Double Deep Q-Network (DQN) on the game of Breakout using the OpenAI Gym.",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("dqn.gif"))
``` 
  
  -  Implemented Deep Q-Network (DQN) and Double DQN on the game of Breakout using the OpenAI Gym.
  -  Double DQN uses a target network to calculate the target Q-value (next state maximum) whereas the vanilla DQN uses the same network for computing Q values for both the current and next state.
  -  Please find the details of the Python code used  [here](https://gitfront.io/r/user-2013142/0fbc0374f87206739e93444c1e6e3a8750c93eef/Deep-Reinforcement-Learning/). 
  -  All calculations were run on Google Colab. Due to limitations in availability of computational resources, the model could be trained only for 500 episodes.


<hr style="border:2px solid gray"> </hr>
### YOLO Object Detection on PASCAL VOC[<span style="color:blue"> [GitFront]</span>](https://gitfront.io/r/user-2013142/441c69af3daf8e73e7e8b05da84f221e5382d39b/YOLO-Object-Detection/)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap="Results obtained from YOLO",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("download_lol.png"))
```  
  -  Implemented a YOLO-like object detector on the PASCAL VOC 2007 data set to produce results as shown in the above image.
  -  Used a pre-trained network structure for the model. In particular, we used the ResNet50 architecture as a base for our detector. This is different from the base architecture in the YOLO paper and also results in a different output grid size (14x14 instead of 7x7).
  -  Please find the details of the Python code used  [here](https://gitfront.io/r/user-2013142/441c69af3daf8e73e7e8b05da84f221e5382d39b/YOLO-Object-Detection/). 
  -  All calculations were run on Google Colab.



<!-- ### **Face Generation with GANs** -->
<hr style="border:2px solid gray"> </hr>
### Face Generation with GANs[<span style="color:blue"> [GitFront]</span>](https://gitfront.io/r/user-2013142/fafae6dc10869ea5e5b85f1efbce3ae8a99a5f2e/Generative-Adversarial-Network/)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 95%",out.height="95%",fig.cap=" Deep Convolutional Generative Adversarial Network Architecture. Ref: https://gluon.mxnet.io/chapter14_generative-adversarial-networks/dcgan.html",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("dcgan.png"))
``` 

  -  Trained a generative adversarial network (GAN) with spectral normalization of discriminator weights on the CelebA Dataset to generate facial images.
  -  The generator and discriminator network architectures  implemented are roughly based on DCGAN.
  -  Spectral normalization of discriminator weights is used to improve the quality of generator images.
  -  Both DCGAN and LSGAN loss functions were used while keeping the network architecture constant (with and without spectral normalization).
  -  Please find the details of the Python code used  [here](https://gitfront.io/r/user-2013142/fafae6dc10869ea5e5b85f1efbce3ae8a99a5f2e/Generative-Adversarial-Network/). 
  -  All calculations were run on Google Cloud. Due to limitations in availability of computational resources, the model could be trained only for 22 epochs. Some results are shown below.



```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap=" Results obtained from LSGAN and DSGAN",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("gan_results_1.png"))
```
```{r, echo=FALSE, out.width=" 55%",out.height="55%",fig.cap="Effect of spectral normalization on the quality of predicted images ",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("spec_norm.png"))
```




<hr style="border:2px solid gray"> </hr>
<!-- ### **Text Generation & Language classification (RNNs)** -->
### Text Generation & Language classification (RNNs)[<span style="color:blue"> [GitFront]</span>](https://gitfront.io/r/user-2013142/e8b804a819493bc8d2501a85f7620ba5113f6b68/Recurrent-Neural-Networks/)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 45%",out.height="45%",fig.cap="Visualizing  the performance of the RNN  model by creating a confusion matrix. The ground truth languages of samples are represented by rows in the matrix while  predicted languages are represented by the columns.",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("confusion.png"))
```

  - Trained and implemented an RNN for two tasks on text data:
    - Text generation - RNN learns to generate text by predicting the most likely next character based on previous characters. The model consists of three layers 
      - a linear layer that encodes the input character into an embedded state, 
      - an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and
      - a decoder layer that outputs the predicted character scores distribution.
      - The model was trained on the completer works of Shakespeare and 24 different novels of Charles Dickens. 
    - Language classification - RNN learnt to detect which language a chunk of text is written in (similar to a feature you might find in an online translator). 
      - While this might be relatively easy to do if the input text is Unicode and unique characters indicate a particular language, however this implementation addressed the case where all input text were converted to ASCII characters so our network learnt instead to detect letter patterns. 
      - The model was trained on the Bible which is a large text translated to different languages but is in easily parsable format, so 20 different copies of the Bible in different languages obtained from Project Gutenberg were used.
    
**Sample output generated by the RNN after training on Shakespeare is shown below.**

```
TRIA:
He dew that with merry a man for the strange.
I then to the rash, so must came of the chamuness, and that I'll treason dost
the heaven! how there. The run of these thou instress
Which wast true come come on my tongue.

KATHARINE:
My lord, the crown English am a thanks, and I
have you weep you galls. O, I wast thy change;
And go turn of my love to the master.'

ARCHBISHOP OF YORK:
I'll find by dogs, noble.

SAMLET:
The matter were be true and treason
Free supples'd best the soldiered.

TITUS ANDRONICUS:
I ever a bood;
But one a stand have a court in thee: which man as thy break on my bed
'As oath a women; there and shake me; and whencul, comes the house
For them he wall; and no live away. Fies, sir.
```
<hr style="border:2px solid gray"> </hr>
### Multi Label Image Classification[<span style="color:blue"> [GitFront]</span>](https://gitfront.io/r/user-2013142/51814a2a409aed6b9b49e7a8218754a4f2ce1167/Multi-Label-Image-Classification/)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 45%",out.height="45%",fig.cap="A sample image from the PASCAL VOC 2007  dataset",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("pascal_voc.png"))
```
  - Implemented a multi-label image classifier on the [<span style="color:blue">PASCAL VOC 2007 </span>](http://host.robots.ox.ac.uk/pascal/VOC/voc2007) data set. Carried out three experiments:
    - Trained AlexNet (PyTorch built-in) from scratch.
    - Fine-tuned AlexNet (PyTorch built-in), which was pretrained on [<span style="color:blue">ImageNet.</span>](http://www.image-net.org/)
    - Trained a simple network (defined in the file classifier.py) from scratch.
  - In the second part, designed and trained my own deep simple convolution network to predict a binary present/absent image-level label for each of the 20 PASCAL classes. The network was designed with AlexNet as a starting point. The goal was to get an average mAP score of at least 40 percent. 
  
**The average precision on different classes are shown below after 55 epochs on Google Cloud (16GB RAM and 1 NVIDIA K80 GPU).**

```
-------  Class: aeroplane        AP:   0.6468  -------
-------  Class: bicycle          AP:   0.4247  -------
-------  Class: bird             AP:   0.3491  -------
-------  Class: boat             AP:   0.3989  -------
-------  Class: bottle           AP:   0.1596  -------
-------  Class: bus              AP:   0.2318  -------
-------  Class: car              AP:   0.6456  -------
-------  Class: cat              AP:   0.3552  -------
-------  Class: chair            AP:   0.4179  -------
-------  Class: cow              AP:   0.2235  -------
-------  Class: diningtable      AP:   0.3586  -------
-------  Class: dog              AP:   0.3028  -------
-------  Class: horse            AP:   0.6846  -------
-------  Class: motorbike        AP:   0.5332  -------
-------  Class: person           AP:   0.7901  -------
-------  Class: pottedplant      AP:   0.2159  -------
-------  Class: sheep            AP:   0.2858  -------
-------  Class: sofa             AP:   0.2924  -------
-------  Class: train            AP:   0.5996  -------
-------  Class: tvmonitor        AP:   0.2998  -------
mAP: 0.4108
Avg loss: 0.1801034240768506
```
<hr style="border:2px solid gray"> </hr>
### Multi Layer Neural Networks[<span style="color:blue"> [GitFront]</span>](https://gitfront.io/r/user-2013142/ea0abbc5ec0932dfd20f167b254e347e9b3d750f/Multi-Layer-Neural-Networks/)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width=" 45%",out.height="45%",fig.cap="A sample image from the CIFAR-10 image classification dataset. Ref: https://www.cs.toronto.edu/~kriz/cifar.html",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("cifar_10.png"))
```
  - Implemented multi-layer neural networks (2 and 3 layers) from scratch on the [<span style="color:blue"> CIFAR-10 </span>](https://www.cs.toronto.edu/~kriz/cifar.html) image classification data set, to understand the fundamentals of neural networks and backpropagation.

  - Developed codes for forward and backward pass, and trained two- and three-layer networks with SGD and Adam optimizer.
  
  - Had experience with hyperparameter tuning and using proper train/test/validation data splits.
  

```{r, echo=FALSE, out.width=" 75%",out.height="75%",fig.cap="Results of using the SGD and Adam optimzers on the CIFAR-10 dataset. Note that Adam converges faster than SGD in the loss history plot which is expected. We also see overfitting in the classification plot given that our dataset was realtively small compared to the number of NN parameters.",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("adam.png"))
```

<hr style="border:2px solid gray"> </hr>
### Ad Hoc Information Retrieval System[<span style="color:blue"> [GitHub]</span>](https://github.com/punitjha/Small_ML_NLP_Projects/tree/master/Ad_Hoc_Info)
<hr style="border:2px solid gray"> </hr>
<!-- ### **Ad Hoc Information Retrieval System**   -->
```{r, echo=FALSE, out.width=" 65%",out.height="65%",fig.cap=" Architecture of an ad hoc IR system. Source: Jurafsky and Martin 2009, sec. 23.2",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("info.png"))
```
  - Using 1400 abstracts of journal articles created an information retrieval system that takes in a query and outputs the top matched abstracts using TF-IDF weights and cosine similarity scores.
  - The model was trained on the  [<span style="color:blue"> Cranfield collection </span>](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  This was the pioneering test collection in allowing precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments. It contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs.
  - This project was a part of the ECE 365: Data Science and Engineer course.
  - More details of the project can be found in this repository [here](https://github.com/punitjha/Small_ML_NLP_Projects/tree/master/Ad_Hoc_Info)





<hr style="border:2px solid gray"> </hr>
### Document Clustering [<span style="color:blue"> [GitHub]</span>](https://github.com/punitjha/Small_ML_NLP_Projects/blob/master/Document_Clustering/Document_Clustering.ipynb)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="70%",out.height="70%",fig.cap=" Principal components of feature data and the clusters  obtained (kmeans and hierarchical clustering) using Scikit learn's PCA",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("clustering.png"))
```

In this project, we worked with abstracts of research papers published on different aspects of coronaviruses over the years. Our goal was to segement the abstracts into different clusters based on the similarities in the topics that the abstracts talk about.

  - First, used TF-IDF vectorizer to create a feature space to represent the abstracts.
  - Used kmeans and hierarchical clustering to creates 3 different clusters and visualize them.
  - Analyzed parameters like intra-cluster and inter-cluster distances to access the efficacy of the clustering that we have done.
  - Obtained the most important words for each of the clusters that we had created.

<hr style="border:2px solid gray"> </hr>
### Machine Learning Algorithms Using only Numpy
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="500%",out.height="500%",fig.cap=" (a) Comparison of SGD with Momentum or Nesterov Accelerated Gradient, and Adam methods and their rate of convergence. (b) Image compression using PCA",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("index_both.png"))
```

- Implemented ML algorithms from scratch using only Python Numpy. All of these algorithms are run on tiny data sets on a standard CPU laptop. These were part of assignments from CS-446 and CS-498 at UIUC.
  - Stochastic Gradient Descent Variants - SGD with Momentum or Nesterov Accelerated Gradient, and Adam. [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - GMM [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - HMM [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - PCA [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - RNN [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - Adaboost On Stumps [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - Feed Forward Neural Network [<span style="color:blue">[GitHub]</span>](https://github.com/punitjha/ML-from-Scratch)
  - kNN, LDA and Bayes Classifiers and comparison with corresponding scikit-learn modules. [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab2_vvv/punit2_lab2.ipynb)
  - Mulit-layer Neural Networks (2 and 3 layered NN with SGD and Adam) [<span style="color:blue"> [GitHub] </span>](https://github.com/punitjha/Multi-Layer-Neural-Networks)
  - Logistic regression   [<span style="color:blue"> [GitFront] </span>](https://gitfront.io/r/user-2013142/4f2fa8a5fd85f0e24f1599dab48de75c7d77a387/Linear-Classifiers/)
  - Perceptron   [<span style="color:blue"> [GitFront] </span>](https://gitfront.io/r/user-2013142/4f2fa8a5fd85f0e24f1599dab48de75c7d77a387/Linear-Classifiers/)
  - SVM   [<span style="color:blue"> [GitFront] </span>](https://gitfront.io/r/user-2013142/4f2fa8a5fd85f0e24f1599dab48de75c7d77a387/Linear-Classifiers/)
  - Softmax  [<span style="color:blue"> [GitFront] </span>](https://gitfront.io/r/user-2013142/4f2fa8a5fd85f0e24f1599dab48de75c7d77a387/Linear-Classifiers/)
 

<hr style="border:2px solid gray"> </hr>
### Sentiment Analysis [<span style="color:blue"> [GitHub]</span>](https://github.com/punitjha/Small_ML_NLP_Projects/blob/master/Sentiment%20Analysis/Sentiment%20Analysis.ipynb)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="70%",out.height="70%",fig.cap=" Principal components of feature data and the clusters  obtained (kmeans and hierarchical clustering) using Scikit learn's PCA",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("sent_anal.png"))
```

In this project, we worked with IMDB movie reviews and developed different machine learning models to predict a given review as positive or negative.

  -  Data  cleaning was first carried out (removing stopwords, punctuation and digits).
  -  Used two different approaches to create the feature space. First, we considered the presence/absence of the word (feature) into account. We used 2 classification models, SVM and random forest. Calculated the f1 score, test set accuracy and 10 most ifluential words for each of the models.
  - In the 2nd one, we will used TF-IDF vectorization and also includde bigrams in the analysis. We built naive bayes and random forest classifier. Calculated the f1 score, test set accuracy and 10 most ifluential words for each of the models. Also used cross-validation and grid search approach to tune the random forest model.


<hr style="border:2px solid gray"> </hr>
### Topic Modelling [<span style="color:blue"> [GitHub]</span>](https://github.com/punitjha/Small_ML_NLP_Projects/blob/master/Topic_Modelling/Topic_Modelling.ipynb)
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="60%",out.height="60%",fig.cap="An illustration of topic modelling Ref:[Medium article](https://medium.com/analytics-vidhya/how-to-perform-topic-modeling-using-mallet-abc43916560f) ",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("topic_modelling.png"))
```
In this project, we worked with research papers published on different aspects of coronaviruses over the years. Our goal was to use topic modelling to know different areas each research paper talks about and answer some important questions regarding the viruses.

  - First  we extractied full body text, abstract and title from each paper and cleaned them.
  - Used gensim library to create a LDA topic model on the extracted body texts.
  - Used topic modelling and tried to find most relevant papers on aspects like vaccine and respiratory viruses.
  


<hr style="border:2px solid gray"> </hr>
###  Machine Learning, NLP, and Text Analysis using Scikit-learn
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="200%",out.height="200%",fig.cap="Use of [spectral clustering](https://arxiv.org/pdf/0711.0189.pdf) to separate data that K-means cannot. (a) Original data (b) Clustering using K-means (c) Spectral Clustering",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("download.png"))
```
- This is a collection of jupyter notebooks that were used to learn Data Science and ML as a part of ECE 365: Data Science & Eng. course. This involved using modules and methods avaliable in the scikit-learn package for various ML tasks and comparing its output againt results obtained from codes written from scratch. The Ocatave repos are from the ML course by Andrew Ng.
  - **Machine Learning**
      <!-- - k-Nearest Neighbors, Bayes Classifiers, and Linear Discriminant Analysis. [[Jupyter Notebook Link]]() -->
      - Naive Bayes, Logistic Regression, and Support Vector Machines. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab3_vvv/punit2_lab3.ipynb)
      - K-means clustering, Vector Quantization, Nearest Neighbors Classification, and Linear Regression. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab4_vvv/punit2_lab4.ipynb)
      - Eigen decomposition, Singular Value Decomposition, and Principal Component Analysis.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Lab5_vvv/punit2_lab5.ipynb)
     - Intro to Octave, Linear regression with one variable, Gradient Descent, and Linear regression with multiple variables.[[Octave ]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex1/ex1)
    - Logistic Regression, Regularized logistic regression, and Plotting the decision boundary. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex2/ex2)
    - Multi-class Classification/ One-vs-all Classification and Neural Networks. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex3/ex3)
    - Neural Networks, Backpropagation, and Visualizing the hidden layer. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex4/ex4)
    - Regularized Linear Regression, Bias-variance and learning curves, Polynomial regression, Adjusting the regularization parameter, Computing test set error, and visualization.
[[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex5/ex5)
    - Support Vector Machines and Spam Classification. [[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex6/ex6)
    - K-means Clustering and Image compression with K-means.
[[Octave]](https://github.com/punitjha/Machine_Learning_Coursera/tree/master/machine-learning-ex7/ex7)
  - **Natural Language Processing**
      - Text processing and regular expressions, word frequencies and language, manipulate corpora and plot insightful graphs. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab1/punit2/Lab1/punit2.ipynb)
      - Tokenization, bag-of-words representation, logistic regression classifier (discriminative model) using PyTorch.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab2/punit2/NLPLab2/Lab2_NLP.ipynb)
      - N-gram model, generate representative sentences, study of the effect of training data size, and language model complexity (n-gram size), on the modeling capacity of a language model.
[[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab3/punit2/NLPLab3/NLPLab3.ipynb)
      - Count-Based Word Vectors, Co-occurrence_matrix, PCA, plot-embeddings,  Word2Vec Word Embeddings, Cosine Similarity, Polysemous Words, Synonyms & Antonyms, Solving Analogies with Word Vectors, Guided Analysis of Bias in Word Vectors, and Independent Analysis of Bias in Word Vectors. [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/NLP_Lab4/punit2/Lab4/lab4.ipynb)
<!--   - **Text Analysis** -->
<!--       - Syntagmatic word association mining. -->
<!-- [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-1/punit2_lab1.ipynb) -->
<!--       - Sentiment analysis. -->
<!-- [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-2/punit2_lab2.ipynb) -->
<!--       - Document Clustering using TF-IDF vectorizer. -->
<!-- [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-3/punit2_lab3.ipynb) -->
<!--       - Topic Modelling: Use of Gensim library to create a LDA topic model. -->
<!-- [[Jupyter Notebook Link]](https://github.com/punitjha/ECE365-Data_Science_and_Engineering/blob/master/Text_Analysis_Lab-4/punit2_lab4.ipynb) -->


 

***
## **Programming Projects**
<hr style="border:2px solid gray"> </hr>

### Parallel Programming Projects
<hr style="border:2px solid gray"> </hr>


```{r, echo=FALSE, out.width="80%",out.height="80%",fig.cap="",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("parallel_1.png"))
```

- **Design and Implementations of Parallel OpenMP and MPI LU factorization: Application to Finite Difference Method.**
  - In this project, I with my group presented  six different OpenMP and
MPI based parallel algorithms for the LU factorization of dense matrices.
  - Please find the details [here](https://drive.google.com/file/d/1I17RTZGtopQQOAkk9z2MUyQEGYWmYsRN/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_Project)
     
- **Optimization of a dense matrix-matrix multiplication routine and understanding its performance behavior.** 
  - Parallel programming machine project on dense matrix-matrix multiplication and cache performance.
  - Please find the details [here](https://drive.google.com/file/d/1IvqxqR6E6_CINKB8OunAmBpQvYcb_nPo/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_MPs)
   
- **Utilization of  multi-core CPU using OpenMP and MPI to implement a dense matrix-matrix multiply (MM) routine and understanding the performance behavior.**
  - Usage of PAPI to measure cache misses and to analyze the performance.
  - Please find the details [here](https://drive.google.com/file/d/1yYzdTwtnV8-4xoFCRmFnWe275Ny0dOx9/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_MPs)
    
- **Utilization of  multi-core CPU using OpenMP and a GPU using OpenACC to implement a matrix-multiply routine for multiplying a band matrix by a dense matrix.**
  - Investigation of the performance impact of varying the band and matrix size.
  - Please find the details [here](https://drive.google.com/file/d/1Pb781ThU_9EG2uDTm1NB4uaETlWU7GH4/view?usp=sharing)
  - Codes [here](https://github.com/punitjha/CS_420_MPs)
   

<hr style="border:2px solid gray"> </hr>
### Data Structures in C++ Projects
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="80%",out.height="80%",fig.cap="Images taken from CS 400 UIUC and [GeeksForGeeks](https://www.geeksforgeeks.org/difference-between-general-tree-and-binary-tree/)",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("lol_ok.png"))
```
- **Image transformation ** 
  - Intro to object oriented programming in C++. It involved constructions of classes, inheritance, memory management etc.
  - Please find the details [here](https://github.com/punitjha/Data_Structures_Cplusplus/blob/master/Image_Transformation/image-transform-instructions.pdf)
  - Codes [here](https://github.com/punitjha/Data_Structures_Cplusplus/tree/master/Image_Transformation/image-transform-given-files)
- **Generic tree and tree traversal**
  - Please find the details [here](https://github.com/punitjha/Data_Structures_Cplusplus/blob/master/Generic_Tree_and_Tree_Traversal_Project/tree_traversal_project_instructions.pdf)
  - Codes [here](https://github.com/punitjha/Data_Structures_Cplusplus/tree/master/Generic_Tree_and_Tree_Traversal_Project/GenericTree_starter_files/GenericTree)
- **Unordered Maps**
  - Please find the details [here](https://github.com/punitjha/Data_Structures_Cplusplus/blob/master/Unordered_Maps/UnorderedMap_project_instructions.pdf)
  - Codes [here](https://github.com/punitjha/Data_Structures_Cplusplus/tree/master/Unordered_Maps/UnorderedMap_starter_files/UnorderedMap)
- **Linked List and Merge Sort**
  - Please find the details [here](https://github.com/punitjha/Data_Structures_Cplusplus/blob/master/Linked_List_Merge_Sort/LinkedList_project_instructions.pdf)
  - Codes [here](https://github.com/punitjha/Data_Structures_Cplusplus/tree/master/Linked_List_Merge_Sort/LinkedList_starter_files/LinkedList)
   
<hr style="border:2px solid gray"> </hr>
### Chemistry Programming Projects
<hr style="border:2px solid gray"> </hr>

<!-- 1. **Programming projects for [CHEM 548: Molecular Electronic Structure Advanced Quantum Chemistry & Numerical Methods.](http://faculty.scs.illinois.edu/hirata/education.html)** -->
<!--    - Third Project -->
<!--      - Hückel and Su-Schrieffer-Heeger band structures for 1D and 2D solids. -->
<!--      - The report can be found [here](https://drive.google.com/file/d/13CjQVDkpwSTJEk4sAuKDva8kw8z89CHt/view?usp=sharing) -->
<!--      - Codes can be found [here](https://github.com/punitjha/CHEM_548_Project_3) -->
<!--    - Second Project -->
<!--      - Wave packet propagation, phase and group velocities, Ehrenfest theorem, convolution theorem, path integrals. -->
<!--      - The report can be found [here](https://drive.google.com/file/d/1_XcSwoqlycy-i354jd4TW_kfLtEWTs8P/view?usp=sharing) -->
<!--      - Codes can be found [here](https://github.com/punitjha/CHEM_548_Project_2) -->
<!--    - First Project -->
<!--      - Time-dependent perturbation theory and spectroscopy. -->
<!--      - The report can be found [here](https://drive.google.com/file/d/1zz3pD4KzFYVm5nen6yhf8u-hqnGXjfF5/view?usp=sharing) -->
<!--      - Codes can be found [here](https://github.com/punitjha/CHEM_548_Project_1) -->

```{r, echo=FALSE, out.width="80%",out.height="80%",fig.cap="",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("chem.png"))
```

- **Electronic structure theory programming projects. ** 
   - Molecular Geometry Analysis.
     - [Instructions](https://drive.google.com/file/d/1ucsnJL2St-DKmW5cs6o_3M0TKxbly09p/view?usp=sharing) and  [Codes](https://github.com/punitjha/Crawdad_Project_4)
   - Harmonic Vibrational Analysis.
     - [Instructions](https://drive.google.com/file/d/1rTu-TCD8vZMX0CsE9NFp1brANDxAqt5x/view?usp=sharing) and  [Codes](https://github.com/punitjha/Crawdad_Project_3)
   - The Hartree-Fock Self-Consistent Field Method.
     -  [Instructions](https://drive.google.com/file/d/1sIRmEjvBEmPc7N1bsYVXyuu1kVwnSsYz/view?usp=sharing) and [Codes](https://github.com/punitjha/Crawdad_Project_2)
   - The Second-Order Möller-Plesset Perturbation Theory (MP2) Energy.
     -  [Instructions](https://drive.google.com/file/d/1zDiOA2GHTFlNxmCZSjk4dJDEQ-xe4-4x/view?usp=sharing) and [Codes](https://github.com/punitjha/Crawdad_Project_1)
   - Hückel and Su-Schrieffer-Heeger band structures for 1D and 2D solids.
     - [Report](https://drive.google.com/file/d/13CjQVDkpwSTJEk4sAuKDva8kw8z89CHt/view?usp=sharing) and  [Codes](https://github.com/punitjha/CHEM_548_Project_3)
   - Wave packet propagation, phase and group velocities, Ehrenfest theorem, convolution theorem, path integrals.
     - [Report](https://drive.google.com/file/d/1_XcSwoqlycy-i354jd4TW_kfLtEWTs8P/view?usp=sharing) and  [Codes](https://github.com/punitjha/CHEM_548_Project_2)
   - Time-dependent perturbation theory and spectroscopy.
     - [Report](https://drive.google.com/file/d/1zz3pD4KzFYVm5nen6yhf8u-hqnGXjfF5/view?usp=sharing) and [Codes](https://github.com/punitjha/CHEM_548_Project_1)




***
## **Chemistry Research Projects and Summer Internships**
<hr style="border:2px solid gray"> </hr>
### Ph.D. Research Project 
<hr style="border:2px solid gray"> </hr>
```{r, echo=FALSE, out.width="80%",out.height="80%",fig.cap="First order corrections to chemical potential, grand potential and internal energy at a finite-temperature in a grand canonical ensemble",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("phd_image.png"))
```
- **University of Illinois at Urbana-Champaign, Urbana-Champaign, IL.**
  - **Ph.D. Student and Teaching Assistant, Aug 2016 - Present**
  - **Summary**:
      - **Finite-temperature many-body perturbation theory (FT-MBPT)**: FT-MBPT is a theory to calculate the thermodynamic parameters of the grand canonical system at a given temperature. 
      - I implemented the $\lambda$-variation method to  calculate the $n$th-order corrections to grand potential and internal energy using the FT-MBPT formalism which showed it is mathematically inconsistent and does not conserve charge. 
      - Derived the correct analytic formulas for FT-MBPT using another ansatz wherein charge was conserved.
      Implemented the FT-MBPT in a canonical ensemble and derived semi-analytic formulas.
  - **Skills Acquired:**
          Acquired knowledge of \textit{ab initio} and DFT electronic structure methods. Learned about machine learning algorithms. Learned to program in Python and enhanced my programming skills in C++ and FORTRAN.
          

#### **Detailed Summary**

 - Please find the details of my Ph.D. project [here](https://drive.google.com/file/d/1gYTqQoy4uCIJNqHBbMloC61JV3wZPn6w/view?usp=sharing). 

  <!-- - Research conducted at the University of Illinois at Urbana-Champaign, Urbana-Champaign, IL, Aug 2016 - Present.  -->
  <!--    - **Topic**: Finite-temperature many-body perturbation theory. -->
  <!--    - More details [here](https://punitjha.github.io/phd_project.html). -->
    
<hr style="border:2px solid gray"> </hr>
### M. Sc. Research Project
<hr style="border:2px solid gray"> </hr>

- Research conducted at the National Institute of Science Education and Research, Bhubaneswar, India, Aug 2014 - June 2016.
    - **Topic**: Diagonal suppressed constant time Correlation Spectroscopy (COSY).
    - More details [here](https://punitjha.github.io/msc_project.html). 
    
<hr style="border:2px solid gray"> </hr>
### Research Summer Internships
<hr style="border:2px solid gray"> </hr>
1. **Undergraduate Summer Intern- 2015**
    - Research conducted at the University of Calgary, Calgary, Canada, May 2015 - Aug 2015.
       - **Topic**: Dynamics of gas-hydrate nucleation.
       -  More details [here](https://punitjha.github.io/calgary.html).
  
2. **Undergraduate Summer Intern- 2014**
     - Research conducted at the Indian Institute of Technology- Bombay, Mumbai, India, May 2014 - Aug 2014.
        - **Topic**: Quantum reactive flux theory.
        - More details [here](https://punitjha.github.io/iit_b.html).
         
      
3.  **Undergraduate Summer Intern- 2013** 
     - Research conducted at the National Institute of Science Education and Research, Bhubaneswar, India, May 2013 - Aug 2013.
         - **Topic**: Reactive molecular dynamics of hydrocarbon dissociation on Ni surface.
         - More details [here](https://punitjha.github.io/niser_summer.html).
          
*** 

  
  <!--   - Reactive molecular dynamics of hydrocarbon dissociation on Ni surface: Carried out reactive dynamics simulations of the adsorption and decomposition of CH\textsubscript{4} molecules interacting with a Ni(100) surface. Studied the temperature dependence of chemisorption of CH\textsubscript{4} by breakage of C-H bonds that leads to the formation of CH\textsubscript{3} and H radical on a Ni(100) surface. -->
  <!-- - **Skills Acquired:** -->
  <!--         Learned programming in FORTRAN 90 and how to carry out quantum mechanical simulations using ReaxFF. -->
             
             
             
<!--     - Finite-temperature many-body perturbation theory (FT-MBPT): FT-MBPT is a theory to calculate the thermodynamic parameters of the grand canonical system at a given temperature.  -->
  <!--     - I implemented the $\lambda$-variation method to  calculate the $n$th-order corrections to grand potential and internal energy using the FT-MBPT formalism which showed it is mathematically inconsistent and does not conserve charge.  -->
  <!--     - Derived the correct analytic formulas for FT-MBPT using another ansatz wherein charge was conserved. -->
  <!--     Implemented the FT-MBPT in a canonical ensemble and derived semi-analytic formulas. -->
  <!-- - **Original Research Proposal (ORP)** -->
  <!--       - Developed an ORP on the problem of inclusion of long-range (electrostatic) interactions in machine learning algorithms. Analyzed different neural network schemes,            especially deep high-dimensional neural networks (HDNNs), and whether they can efficiently learn and predict molecular charges learned from various charge partitioning schemes. -->
  <!--     - Studied different charge partitioning schemes like Hirshfeld, Charge Model 5, Merz-Singh-Kollman, and Natural Bonding Orbital methods used in molecular dynamics              simulations. -->
  <!--     - Proposed modifications to mathematical formulation and structure of HDNNs to be able to better predict  molecular charges, computational cost for implementing the            project, potential  setbacks, and alternate plans for the project. -->
  <!-- - **Skills Acquired:** -->
  <!--         Acquired knowledge of \textit{ab initio} and DFT electronic structure methods. Learned about machine learning algorithms. Learned to program in Python and enhanced           my programming skills in C++ and FORTRAN. -->

  <!-- ---     -->
  
  
  
  
  
  <!--   - Diagonal suppressed constant time Correlation Spectroscopy (COSY): Worked on a novel NMR pulse sequence to achieve diagonal suppressed COSY (correlation spectroscopy) spectra with high sensitivity and absorptive cross peak line-shapes.              -->
  <!--   - Carried out simulations and  product operator  calculations for 2, 3, and 4 spin systems using the novel NMR pulse sequence. -->
  <!--   - Implemented various optimization strategies for the pulse sequence that included the addition of water suppression module, removing axial and multiple quantum peaks by using gradient fields, etc.  -->
  <!-- - **Skills Acquired:** -->
  <!--         Acquired programming skills to program in the TopSpin 2.1 programming language and  learned to how to perform 1D, 2D NMR experiments on a 400 MHz spectrometer. -->

  <!-- ---     -->
  
  
   <!-- - Quantum reactive flux theory: Studied the quantum analog of classical non-Markovian Kramers' equation and the phase space distribution function formulation of the method of reactive flux in the memory friction regime. Analyzed quantum Kramers' equation for energy diffusion and barrier crossing dynamics in the high-friction regime. -->
   <!--  - **Skills Acquired:** -->
   <!--        Acquired skills to use Mathematica for solving integrals and numerical analysis. Learned mathematical methods in physics. -->

   <!--   ---   -->
   
<!--      -->
<!--       - Dynamics of gas-hydrate nucleation: Wrote C++  codes to study the probability distribution of lifetime of gas hydrate cages in the molecular dynamics simulation of  incipient hydrates starting from a CH\textsubscript{4}/H\textsubscript{2}S bubble in liquid H\textsubscript{2}O in a 3D box. -->
<!--       - Studied the effect of the different guest gas molecules on the lifetime of these cages. } -->
<!--   - **Skills Acquired:** -->
<!--           Learned about the object-oriented programming features of C++ and its data structures. I also learned the skills -->
<!-- required to create memory-efficient C++ codes. -->

<!--   ---   -->
  
 